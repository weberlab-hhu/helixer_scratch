\documentclass{bioinfo}
\copyrightyear{2015} \pubyear{2015}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{Manuscript Category}

\begin{document}
\firstpage{1}

\subtitle{Really cool deep learning stuff}

\title[short Title]{General purpose gene annotation using deep learning}
\author[Sample \textit{et~al}.]{Felix Stiehler\,$^{\text{\sfb 1,}}$$^\dagger$, Alisandra Denton\,$^{\text{\sfb 1,}}$$^\dagger$\, Marvin Steinborn\,$^{\text{\sfb 1}}$, Stephan Scholz, Andreas Weber\,$^{\text{\sfb 1,}*}$}
\address{$^{\text{\sf 1}}$Institue for Plant Biochemistry, Heinrich-Heine-University, Dusseldorf, D-40225, Germany}

\corresp{$^\dagger$Authors contributed equally. \newline $^\ast$To whom correspondence should be addressed.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\abstract{\textbf{Motivation:} Current state of the art gene annotation programs for eukaryotic genomes have to be pretrained for each species and still often produce annotations that lack the required accuracy. \\
\textbf{Results:} We set up a framework that can be used to build much more general gene callers that also outperform traditional methods on a variety of complex genomes.\\
\textbf{Availability:} Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text
Text Text Text Text Text Text Text Text Text Text Text Text Text Text  Text\\
\textbf{Contact:} \href{name@bio.com}{name@bio.com}\\
\textbf{Supplementary information:} Supplementary data are available at \textit{Bioinformatics}
online.}

\maketitle

\section{Introduction}
For many tasks, it is necessary to have DNA regions labeled according to their function. This process is called gene annotation and it can be performed in different levels of precision, from simple coding -- non-coding classification to detailed structural labeling, such as enhancer regions or long non-coding RNA sequences in eukaryotes. In this work on eucaryotic genomes, we differentiate between Intergenic regions, Untranslated regions (UTRS, parts at the beginning and end of gene, that are transcribed but not translated), Coding sequences (CDS) and Introns. Because of the sheer size of genomes alone, gene annotation is a very labour intensive process and genomic database providers like NCBI [\citealp{NCBI13}] and Ensembl [\citealp{Ensembl16}] offer gene annotation pipelines. These pipelines integrate experimental data (from e.g. RNA-seq or proteogenomics) with homolog sequences in the database and ab-initio gene predictions. The latter is an attractive approach, because it is cheap and fast. State of the art performance ab-initio gene prediction is achieved by higher order hidden markov models (HMMs), such as Genscan [\citealp{Genscan}] or Augustus [\citealp{Augustus}] but their accuracy leaves room for improvement. By choosing possible states and transitions, designers of HMMs assume structure in the sequence that may limit its predictive power. Indeed, in practice HMMs have problems with generalizing across species and it seems actual learning of underlying patterns is limited. In the last decade, deep neural networks have been applied with tremendous success in many areas of statistical modelling. For sequence data, such as DNA, recurrent neural networks (RNN) in the form of long-short term memory (LSTM) are an established architecture and here, we employ them to improve on HMMs for ab-initio genome annotation.

RNNs have already shown promising results in related, but generally more limited investigations. [\citealp{Choudhary17}] carries out preliminary explorations on the potential of BLSTMs for cross species gene prediction, but trains his model on human genes only and tests it on two more species. DeepAnnotator [\citealp{DeepAnnotator}] uses bidirectional LSTMs (BLSTM) for gene finding in prokaryotes. Gene prediction in prokaryotes is considered more amenable than in eukaryotes, as genes in prokaryotes are proportionately more frequent in the genome, feature simpler control structures and do not use splicing [\citealp{Zhuo04}]. DanQ [\citealp{DanQ}] proposes the use of a BLSTM after a concurrent neural net (CNN) to find detailed motifs in the human genome. DeePromotor[\citealp{DeePromotor}] trains a similar architecture for the recognition of promotor regions. Recently, several groups [\citealp{SpliceAi}, \citealp{SpliceFinder}] successfully used CNNs to find splicing sites.

\section{Approach}
Helixer is a prototype software for training and utilizing a general purpose model for the functional cross-species annotation of large eukaryotic genomes using only DNA data as input. We demonstrate the effectiveness of this approach by training a single model for the annotation of a large set of genomes from the kingdoms metazoa and viridiplantae, respectively, which we will call {\it animals} and {\it plants} from now on. In total we used 192 animal genomes and 60 plant genomes. In contrast to traditional approaches, we employ modelling techniques from the field of deep learning to improve both the annotation accuracy and the ability to generalize across genomes. The source code and all data is publically available.

\begin{methods}
\section{Methods}
We worked with 192 animal and 60 plant genomes. The data of each genome consists of the latest publically available genomic assembly in form of a FASTA file and the latest annotation in the GFF format. We used all animals genomes in EnsemblMetazoa 45 (\citealp{Howe19}) as well as all non-embargoed plant genomes of the JGI Phytosome 13 database (\citealp{Phytozome}). Both data groups were used seperately during training as well as during evaluation. At the start of our work we split both genome groups into {\it training genomes} and {\it test genomes}. The test genome group is made up of 19 or 6 genomes, respectively, and was chosen to be of the best possible quality based on collected metadata while also representing a broad phylogenetic spread. We then experimented which genomes to use for during training from the pool of training genomes while evaluating the performance of models on the remaining ones. We call the genomes that were ultimatily not used during training the {\it validation genomes}. While we did split off a group of test genomes at the beginning of our work, we ended up only reporting on the generalization performance of the combined set of validation and test genomes as the validation genomes are by far the largest set. We also do not find a noticible difference in performance between those two groups. 

We pre-processed and stored the genomic information by using Geenuff (\citealp{Denton19}). Geenuff is a tool for checking and exploring genomic data, which stores all information inside a SQL database. Training and evaluation data was generated by querying this database and transforming the returned data into a numerical format suitable for machine learning. In the case of unambiguous input data, the genomic sequence was transformed into matrix that represents a one hot encoding with 4 rows. For the other case see Supplementary material .. (). The annotations are also transformed into a 4-row one hot encoding matrix, where a one in each of the rows represents one of the possible classes {\it Intergenic}, {\it UTR}, {\it CDS} or {\it Intron}. An non-coding intron can be inferred from the prediction order. A full description of all generated data arrays can be found in table \ref{tab:arrays}.

The classes are not balanced equally and the tradeoff between the number of intergenic and intronic bases is quite different in both groups of genomes. Table \ref{tab:statistics_all} shows the ratio of class occurences, together with other overall metrics.

\begin{table}[!t]
\processtable{Data group statistics for all data\label{tab:statistics_all}} {
\begin{tabular}{@{}lll@{}}
\toprule & Animals & Plants\\
\midrule
Average genome size in Gbp& 2.936 (+- 1.562) & 0.787 (+- 0.995) \\
Average gene length & 31,223 (+- 13,974)& 3,368 (+- 1,510)\\
Geenuff error rate & 0.311 (+- 0.129) & 0.351 (+- 0.249) \\
Fraction of class Intergenic  & 0.777 (+- 0.042) & 0.799 (+- 0.089) \\
Fraction of class UTR & 0.006 (+- 0.006) & 0.017 (+- 0.016) \\
Fraction of class CDS & 0.016 (+- 0.014) & 0.085 (+- 0.071) \\
Fraction of class Intron & 0.201 (+- 0.035) & 0.099 (+- 0.057) \\
\botrule
\end{tabular}}{Values are averages of the individual values of each genome in a data group. All statistics except the average gene lengths are excluding FASTA sequences without a gene and subsequences that were masked as completely erroneous. Each strand of DNA was counted seperately. The average gene length was determined by the average length of the longest transcript by cumulative exon length, which is how we query for genes during data generation. Brackets show the standart deviation.}
\end{table}

During data generation, we only queried for the longest transcript of each gene and did not include FASTA sequences that had no functional annotation (explanation why this is a sign of error?). As Geenuff checks the genomic annotations for potential errors and is able to mark those areas, we were able to effectively blank out those bases during training. The vast majority of bases masked lie in the intergenic region as the most prevelant error is a missing UTR and Geenuff marks a potentially large intergenic region for it. Table \ref{tab:statistics_all} shows statistics about the masking. The corresponding statistics for the selected training genomes only can be found in Section \ref{sec:training_data}. 
	
For the training, we divided each continous genomic sequence into 20.000 bases long subsequences, which are then used as input or label for the neural network. We appended zero padding if the subsequences are not long enough. If a subsequence is fully marked as erroneous, we do not include it anywhere.

The selection of training genomes requires balancing multiple conflicting tradeoffs. On the one hand, we do want to train with as much data as possible, but not all data has a good enough quality to enable a powerful generalization. It is also very desirable to have a broad spread in phylogenetic distance as well as genome size and average gene length as the model is increasingly unlikely to generalize well beyond the borders given by the training data. However, if the genomic patterns inside the data are too different from each other, it may be difficult for the model to generalize as well. It was also important to us to get experimental results within a couple of days as we tested many different data and model configurations, which limited the overall size of the training set and the model. 
	
We employed an iterative approach to effectively select a proper set of training genomes. We first started with 3-4 genomes that were almost certain to be of the highest quality. We then evaluated the performance of the resulting model on all genomes individually (including the training genomes) and looked for candidates to add to the training set or to remove from it. This process was repeated multiple times concurrently with the model search until we saw no more room for substantial improvement given our computational constraints.

Once a set of training genomes was selected, we split it up again into a {\it training and validation set}. This split of the training genomes was done to get a quicker sense of the generalization capabilities and was used primarily during training. We split of the validation set by selecting 20\% of the FASTA sequences above and below the N90 of each training genome, ensuring a proper distribution of large and short sequences in both sets and a split on the chromosome level. Evaluation of the annotations on all training and validation genomes was done regularly after a promising model candidate was found based on its validation set performance. Ultimately we only cared about the cross-species performance of a model.

We use metrics two metrics to judge model performance. The {\it Genic F1} is our primary metric that provides the most comprehensive picture of annotation quality in one number. The {\it Subgenic F1} is similar to the Genic F1, except that it does not take UTR predictions into account. This is done for comparability with AUGUSTUS and further explained in Chapter \ref{augustus}. Both metrics work by summing up True Positives (TP), False Positives (FP) and False Negatives (FN) of each considered class before calculating precision and recall from these values and combining the two into a F1 score. Both metrics do not take the values of the intergenic class into account, as this class is very abundant and appears to be by far the easiest to predict. In the case of the Genic F1, this means that effectively only the TP of the intergenic class are disregarded. The metrics essentially provide a weighted mean of the performances in the considered classes. All individual class performances are reported in Supplementary Material (?). 
	
\subsection{Model Architecture and Usage}
\label{sec:model}
We use a 4-layer deep stacked BLSTM network with 256 units per layer and layer normalization (citation ? ) between each BLSTM layer to produce the predictions in the form of a base pair wise classification. The model consists of circa 5.4 million parameters and was implemented with the deep learning library keras (citation) on top of TensorFlow (citation). We tested multiple different architectures before arriving at this model configuration, including convolutional neural networks (CNN) and hybrid architectures. The hyperparameters were determined by a combination of manual and automatic hyperparameter optimization (talk about TPE?, quote nni?). All relevant hyperparameters can be found in Table \ref{tab:params} or in the Helixer source code repository. 

We also implemented a dilated CNN (dCNN) baseline as this kind of architecture is widely used when working with DNA data as input. Details of the neural architecture search are given in Section \ref{sec:dcnn}.
	
Our final model was trained with 10 bases of genomic sequence as input during each time step and produces individual predictions for each of those 10 bases simultaneously. This grouping enabled us to train effectively with far longer sequences than usual. A class wise evaluation including the calculation of the Genic F1 score is performed on the validation set after every epoch.

We also used multiple techniques to improve the prediction quality after the training was done. One very effective way for genomes with larger genes was to input longer sequences during inference time than during training. This was done for all animal genomes except the invertebrates. The input sequences were up to 10 times larger, depending on species group and assembly quality. This also demonstrates ability of our model to generalize as it is able to make successful prediction on far longer sequences than it has ever seen. The sequence length during inference can be any of 20.000, 50.000, 100.000 or 200.000 and is determined by the species family(?). Families that tend to have longer genes get assigned longer sequences. For more implementation details on this see Section \ref{sec:longer} or the source code.
	
The final predictions of a single model are constructed by overlapping predictions, that were made from a sliding window and then cut to a core sequence. This was done to strongly reduce a typical drop in performance of the models towards the beginning and end of each sequence (see figure \ref{fig:length_wise_bias}) and also to improve model performance in general by providing the model with multiple different starting points. The different overlapping sequences were recombined by averaging the individual softmax values of each base. Supplementary Material (?) contains the influence on overlapping with respect to the position in a sequence for each genome, ordered by N75. We found that overlapping works best if the genomes are not very fragmented.

A model ensemble with 8 components was used to generate the final predictions for each species. For this, we trained 4 seperate animal and plant models and also used a model from an epoch that did not have the highest Genic F1 on the validation set. Depending on the results, we used the model iteration with either the highest genic recall or precision. The model with the best Genic F1 was always best in one of those. This was done to increase the diversity of the model ensemble. As with the overlapping, the fusion of the 8 individual predictions was done by averaging the softmax values of each base pair prediction.

\subsection{Evaluation of AUGUSTUS} \label{sec:augustus}
What Marvin did, why we don't use Genic F1 for comparison, etc


\begin{table}[!t]
\processtable{Data arrays generated and used by Helixer\label{tab:arrays}} {
\begin{tabular}{@{}ll@{}}
\toprule Name & Information \\
\midrule
Input & Genomic sequence in a one hot encoding\\
	  & for (A, T, C, G), otherwise different, see suplementary .. ()\\
Output & Labels in a 4-dimensional one hot encoding \\
Sample weights & One of $\{0,1\}$; whether there is an error at a base\\
\botrule
\end{tabular}}{}
\end{table}

\end{methods}

\begin{figure}[!tpb]
\label{fig:length_wise_bias}
\centerline{\includegraphics[width=0.45\textwidth]{images/length_wise_bias}}
\caption{Average Genic F1 and basepair wise accuracy with respect to the position in the 20000 basepair long input sequences of all animals on top and all plant genomes below shown with and without overlapping. Each value is the average performance of a 200 basepair long chunk by one of the final ensemble models. The regular metrics show a significant negative bias towards the borders of each sequence. Overlapping the predictions helps to mitigate a lot of that while showing no negative side effects elsewhere on average. See Section \ref{sec:overlapping} for a breakout of the effect of overlapping by each individual species.}
\end{figure}

\begin{figure}[!tpb]
	\centerline{\includegraphics[width=0.45\textwidth]{images/overall_boxplot}}
	\caption{Subgenic F1 scores for all animal and plant genomes that were not used during training of Helixer, which is equivalent to all validation and test genomes. Results are given for Helixer and AUGUSTUS with the median predictions scores being denoted by the red lines. Details of how AUGUSTUS was used are given in Section \ref{sec:augustus}.}\label{fig:boxplot}
\end{figure}

\begin{table}[!t]
\processtable{Experimental Results and Comparison\label{tab:results}} {
\begin{tabular}{@{}lll@{}}
\toprule & Animals & Plants\\
\midrule
AUGUSTUS & 0.632 (+-0.09) & 0.757 (+-0.182)\\
Dilated CNN Baseline & - (+-) & - (+-)\\
Best Helixer model & - (+-) & - (+-) \\
Best model + varied input length & - (+-) & - \\
Best model (+ varied input length) + overlapping &  0.85 (+-0.068) & 0.843 (+-0.151) \\
(Varied input length +) overlapping + ensembling & 0.868 (+-0.064) & 0.863 (+-0.148) \\
\botrule
\end{tabular}}{Values are the median in Subgenic F1 across all validation and test genomes of the respective group. Varied input length was only used in the animal case. The best model was chosen out of the eight models of the ensemble for having the best performance in the reported setup.}
\end{table}

\section{Results}
Figure \ref{fig:boxplot} shows the side-by-side comparison of the distribution of performances on all non-training genomes in the animal and plant case by Subgenic F1. In the case of Helixer, these scores represent cross-species predictions. We also compare the median performances of different configurations of Helixer with AUGUSTUS and a dilated CNN baseline in Table \ref{tab:results}. The results show a clear improvement over the AUGUSTUS both in median performance and spread. We also significantly outperform a baseline constructed of a dilated CNN architecture. 

Two techniques were used during inference to improve performance and limit model bias. The usage of longer input lengths helped especially for genomes that tend to have longer genes and was enabled by our model architecture being a relatively simple LSTM stack without any fully connected layers on top. We also constructed the final predictions out of a overlapping ones, which greatly helped to reduce prediction bias in most genomes. To our knowledge, both techniques were not used before in a model developed for gene annotation. 

\section{Conclusion}
We propose a gene calling model that is able to perform cross-species genomic annotiation for a wide variety of large eucaryotic genomes. Our model does this while outperforming AUGUSTUS on a base pair wise level and withstanding RNASeq data based scrutiny. 

\section*{Acknowledgements}

We are very grateful for the support of...

\section*{Funding}

This work has been supported by the... Text Text  Text Text.\vspace*{-12pt}

%\bibliographystyle{natbib}
%\bibliographystyle{achemnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{abbrv}
%\bibliographystyle{bioinformatics}
%
%\bibliographystyle{plain}
%
%\bibliography{Document}


\begin{thebibliography}{}

\bibitem[Thibaud-Nissen {\it et~al}, 2013]{NCBI13}
Thibaud-Nissen,F., Souvorov,A., Murphy,T., DiCuccio,M., Kitts,P. (2013) Eukaryotic Genome Annotation Pipeline{\it The NCBI Handbook, 2nd}

\bibitem[Aken {\it et~al}, 2016]{Ensembl16}
Aken,B., Ayling,S., Barrell,D., Clarke,L., {\it et~al.} (2016) The Ensembl gene annotation system, {\it Database}, {\bf 2016}, baw093

\bibitem[Burge {\it et} Karlin, 1997]{Genscan}
Burge,C., Karlin,S. (1997) Prediction of complete gene structures in human genomic DNA, {\it Journal of Molecular Biology}, {\bf 268-1}, 78-94

\bibitem[Stanke {\it et} Waack, 2003]{Augustus}
	Stanke,M., Waack,S., (2003) Gene prediction with a hidden Markov model and a new intron submodel, {\it Bioinformatics}, {\bf 19 Suppl.2}, ii215–ii225

\bibitem[Zhuo {\it et~al}., 2004]{Zhuo04}
Zhuo,W., Yazhu,C., Yixue,L. (2004) A Brief Review of Computational Gene Prediction Methods, {\it Genomics, Proteomics \& Bioinformatics}, 216-221

\bibitem[Amin {\it et~al}., 2018]{DeepAnnotator}
Amin,M., Yurovsky,A., Tian,Y., Skiena,S. (2018) DeepAnnotator: Genome Annotation with Deep Learning, {\it Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics}, 254-259.

\bibitem[Choudhary, 2017]{Choudhary17}
Choudhary,S. (2017) Predicting protein coding boundaries using RNNs, http://www.saket-choudhary.me/rnn-cds-prediction/predicting-protein-coding.pdf (accessed 2020-04-11)

\bibitem[Daniel {\it et~al}., 2016]{DanQ}
Daniel,Q., Xiaohui,X., (2016) DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences, {\it Nucleic Acids Research}, {\bf 44-11}, e107

\bibitem[Oubounyt {\it et~al}., 2019]{DeePromotor}
Oubounyt,M., Louadi,Z., Tayara,H., Chong2,K. (2019) DeePromoter: Robust Promoter Predictor Using Deep Learning., {\it Frontiers in genetics}, {\bf 10}, 286

\bibitem[Jaganathan {\it et~al}., 2019]{SpliceAi}
Jaganathan,K., Panagiotopoulou,S., McRae,J., Darbandi,S., et.al., Predicting Splicing from Primary Sequence with Deep Learning, {\it Cell}, {\bf 176-3}, 535-548.e24

\bibitem[Wang {\it et~al}., 2019]{SpliceFinder}
Wang,R., Wang,Z., Wang,J. (2019) SpliceFinder: ab initio prediction of splice sites using convolutional neural network, {\it BMC Bioinformatics}, {\bf 20}, 652

\bibitem[Gupta {\it et~al}., 2017]{dCNNGupta}
Gupta,A., Rush.,A. (2017) Dilated convolutions for modeling long-distance genomic dependencies, {\it arXiv preprint arXiv:1710.01278}

\bibitem[Denton {\it et~al}., 2019]{Denton19}
Denton,A., Stiehler,F. (2019) Article title, {\it Journal Name}, {\bf 199}, 133-154.

\bibitem[Howe {\it et~al}., 2019]{Howe19}
Howe KL, Contreras-Moreira B, De Silva N, Maslen G, Akanni W, Allen J, Alvarez-Jarreta J, Barba M, Bolser DM, Cambell L, Carbajo M, Chakiachvili M, Christensen M, Cummins C, Cuzick A, Davis P, Fexova S, Gall A, George N, Gil L, Gupta P, Hammond-Kosack KE, Haskell E, Hunt SE, Jaiswal P, Janacek SH, Kersey PJ, Langridge N, Maheswari U, Maurel T, McDowall MD, Moore B, Muffato M, Naamati G, Naithani S, Olson A, Papatheodorou I, Patricio M, Paulini M, Pedro H, Perry E, Preece J, Rosello M, Russell M, Sitnik V, Staines DM, Stein J, Tello-Ruiz MK, Trevanion SJ, Urban M, Wei S, Ware D, Williams G, Yates AD, Flicek P. (2019) Ensembl Genomes 2020-enabling non-vertebrate genomic research. {\it Nucleic Acids Research 2019} https://doi.org/10.1093/nar/gkz890

\bibitem[Goodstein {\it et~al}., 2012]{Phytozome}
	David M. Goodstein, Shengqiang Shu, Russell Howson, Rochak Neupane, Richard D. Hayes, Joni Fazo, Therese Mitros, William Dirks, Uffe Hellsten, Nicholas Putnam, Daniel S. Rokhsar (2012) Phytozome: a comparative platform for green plant genomics, {\it Nucleic Acids Research}, Volume 40, Issue D1, Pages D1178–D1186, https://doi.org/10.1093/nar/gkr944

\end{thebibliography}

\newpage

\section{Supplementary Material}

\subsection{Dilated CNN Baseline Network Search}
\label{sec:dcnn}
\begin{table}[!t]
\processtable{Parameter search space for the dilated CNN baselines\label{tab:dcnn}} {
\begin{tabular}{@{}ll@{}}
\toprule Parameter & Possible Values \\
\midrule
Kernel Size & \{4, 8, 12, 16\}\\
Initial Filter Depth & \{32, 64, 96, 128\}\\
Number of Layers Before Doubling Filter Count & \{1, 2\}\\
Dilation Multiplier & \{2, 3\}\\
Number of Convolutional Layers & \{2, 3, 4, 5, 6, 7, 8\}\\
Number of Hidden Fully Connected (FC) Layers & \{0, 1, 2\}\\	
Dropout Used on FC and Final Conv Layer Output & \{0.0, 0.01, 0.1, 0.2, 0.3\}\\
Learning Rate & \{1e-3, 1e-4\}\\

\botrule
\end{tabular}}{Above are all parameters used during neural architecture search for the dilated CNN baseline. The same overall space was used for plant and animal data. We did, however, run multiple distinct searches that sometimes only operated over a subset of the given parameters. This was done as those seemed to be the most promising. We for example restricted the search space of the number of LSTM layers to the highest 3 values in later runs. Decisions were guided by the Genic F1 on the validation set of exactly the same data we trained our final LSTM models with. Runs with a Genic F1 below 0.5 after 10 epochs were stopped and the overall maximum epoch was 15. The performances seemed to be leveling off well before that. The batch size used for almost all runs was 32, the dilation was capped to 81 and the size of each fully connected layer was fixed at 128. New parameters where chosen at random. The implementation and parameter space is very roughly based on [\citealp{dCNNGupta}] and can be found in the Helixer source code repository. In total, we trained 30 models for the animals and 48 with the plant data.}
\end{table}

\begin{table}[!t]
\processtable{Final Parameters of dilated CNN baselines\label{tab:dcnn_best}} {
\begin{tabular}{@{}lll@{}}
\toprule Parameter & Animals & Plants\\
\midrule
Kernel Size & 16 & 16\\
Initial Filter Depth & 96 & 32\\
Number of Layers Before Doubling Filter Count & 2 & 2\\
Dilation Multiplier & 3 & 3\\
Number of Convolutional Layers & 6 & 8\\
Number of Hidden Fully Connected (FC) Layers & 2 & 0\\
Dropout Used on FC Output & 0.0 & 0.0\\
Learning Rate & 1e-4 & 1e-3\\
\botrule
\end{tabular}}{The parameters of the dilated CNN model that performed the best according to the Genic F1 of the validation data of the respective training genomes. The best animal model has circa 4.6 million parameters; the best plant model around 2.1 million.}
\end{table}


\subsection{Input Data Encoding}


\subsection{Hyperparameters}

\begin{table}[!t]
\processtable{Hyperparameters\label{tab:params}} {
\begin{tabular}{@{}ll@{}}
\toprule Name & Value \\
\midrule
Layers & 4\\
Units per layer & 256\\
Learning rate & 1e-4\\
Optimizer & Adam\\
\botrule
\end{tabular}}{}
\end{table}

\begin{table}[!t]
\processtable{Class weights\label{tab:params_cw}} {
\begin{tabular}{@{}lll@{}}
\toprule Class & Animal Value & Plant Value \\
\midrule
Intergenic & 0.7 & 0.3 \\
UTR & 1.6 & 1.0 \\
CDS & 1.2 & 0.9 \\
Intron & 1.2 & 0.3 \\
\botrule
\end{tabular}}{The class weights are the only hyperparameters that are different for animals and plants. The exact weights were determined manually and partially reflect the difference in the class distributions of both datasets (see Table \ref{tab:statistics_all}). All other hyperparameters are shown in Table \ref{tab:params}.}
\end{table}


\subsection{Training Data statistics}
\label{sec:training_data}
\begin{table}[!t]
\processtable{Data group statistics for our training data\label{tab:statistics_training}} {
\begin{tabular}{@{}lll@{}}
\toprule & Animals & Plants\\
\midrule
Average genome size in Gbp& 2.489 (+- 2.073) & 0.914 (+- 0.934) \\
Average gene length & 25,672 (+- 16,605) & 3,509 (+- 906)\\
Geenuff error rate & 0.253 (+- 0.138) & 0.134 (+- 0.072) \\
Fraction of class Intergenic & 0.752 (+- 0.05) & 0.808 (+- 0.106) \\
Fraction of class UTR & 0.013 (+- 0.011) & 0.033 (+- 0.023) \\
Fraction of class CDS & 0.028 (+- 0.028) & 0.077 (+- 0.052) \\
Fraction of class Intron  & 0.207 (+- 0.023) & 0.083 (+- 0.037) \\
\botrule
\end{tabular}}{The description of Table \ref{tab:statistics_all} applies here as well.}
\end{table}

\subsection{Longer Sequence Input}
\label{sec:longer}
Implementation detail:
If the N75 of a species is less than twice as high as the supposed sequence length it is lowered until either this criteria is met or a length of 50.000 is reached. 

Table of species with their length.

\subsection{Overlapping evaluation by species}
\label{sec:overlapping}
\begin{figure}[!tpb]
% \centerline{\includegraphics[height=0.01\paperheight]{images/overlapping/animals_overlapping}}
\caption{Caption, caption.}\label{fig:animals_overlapping}
\end{figure}




\end{document}
