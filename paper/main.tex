\documentclass{bioinfo}
\copyrightyear{2015} \pubyear{2015}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{Manuscript Category}

\begin{document}
\firstpage{1}

\subtitle{Really cool deep learning stuff}

\title[short Title]{General purpose gene annotation using deep learning}
\author[Sample \textit{et~al}.]{Felix Stiehler\,$^{\text{\sfb 1,}}$$^\dagger$, Alisandra Denton\,$^{\text{\sfb 1,}}$$^\dagger$\, Marvin Steinborn\,$^{\text{\sfb 1}}$, Andreas Weber\,$^{\text{\sfb 1,}*}$}
\address{$^{\text{\sf 1}}$Institue for Plant Biochemistry, Heinrich-Heine-University, Dusseldorf, D-40225, Germany}

\corresp{$^\dagger$Authors contributed equally. \newline $^\ast$To whom correspondence should be addressed.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\abstract{\textbf{Motivation:} Current state of the art gene annotation programs for eukaryotic genomes have to be pretrained for each species and still often produce annotations that lack the required accuracy. \\
\textbf{Results:} We set up a framework that can be used to build much more general gene callers that also outperform traditional methods on a variety of complex genomes.\\
\textbf{Availability:} Text  Text Text Text Text Text Text Text Text Text  Text Text Text Text
Text Text Text Text Text Text Text Text Text Text Text Text Text Text  Text\\
\textbf{Contact:} \href{name@bio.com}{name@bio.com}\\
\textbf{Supplementary information:} Supplementary data are available at \textit{Bioinformatics}
online.}

\maketitle

\section{Introduction}
\begin{itemize}
	\item Calling genes correctly is very important
	\item but also very difficult in eukaryotes
	\item All current state of the art gene callers work based on HMMs
	\item The structure of the HMMs is finely tuned and engineered
	\item Actual learning is very limited, and with that generalization across different genomes 
	\item (Also have problems with genes that contain long introns, common in mammals)
\end{itemize}

\section{Approach}
Helixer is a prototype software for training and utilizing a general purpose gene annotation model for eukaryotic genomes. We demonstrate the effectiveness of this approach by training a single model for animals (phylogenetic names?) and plants respectively, which was then evaluated on 192 genomes in the animal case and 60 genomes for the plant model. We leverage modelling techniques from the field of deep learning, which has been shown to have the potential to be superior in flexibility and accuracy to traditional machine learning methods in a variety of fields. Our model expects the raw genomic sequence as main input and produces functional predictions on a base pair wise level. The source code and all data is publically available.

\begin{methods}
\section{Methods}
We worked with 192 animal and 60 plant genomes. The data of each genome consists of the latest publically available genomic assembly in form of a FASTA file and the latest annotation in the GFF format. We used all animals genomes in EnsemblMetazoa 45 (?) \citealp{Howe19} as well as those of all non-embargoed plant genomes of the JGI Phytosome 13 database (?, citation). Both data groups were used seperately during training as well as during evaluation. At the start of our work we split both genome groups into training and test sets, which then contained 19 or 6 genomes, respectively. Some of the remaining genomes were used for training purposes and all were used for evaluating our models. 

We pre-processed and stored the genomic information by using Geenuff \citealp{Denton19}. Geenuff is a tool for checking and exploring genomic data, which stores all information inside a SQL database. Training and evaluation data was generated by querying this database and transforming the returned data into a numerical format suitable for machine learning. In the case of unambiguous input data, the genomic sequence was transformed into matrix that represents a one hot encoding with 4 rows. For the other case see Suplementary material .. (). The annotations are also transformed into a 4-row one hot encoding matrix, where a one in each of the rows represents one of the possible classes {\it Intergenic}, {\it UTR}, {\it CDS} or {\it Intron}. An non-coding intron can be inferred from the prediction order. A full description of all generated data arrays can be found in table \ref{Tab:01}.

The classes are not balanced equally and the tradeoff between the number of intergenic and intronic bases is quite different in both groups of genomes. Table \ref{Tab:statistics} shows the ratio of class occurences, together with other overall metrics.

\begin{table}[!t]
\processtable{Data group statistics\label{Tab:statistics}} {
\begin{tabular}{@{}lll@{}}
\toprule & EnsemblMetazoa & JGI Phytosome\\
\midrule
Fraction of classes  & 0.6, 0.02, 0.1, 0.2 & 0.6, 0.02, 0.1, 0.2 \\
(Intergenic, UTR, CDS, Intron) & & \\
Geenuff error rate & 0.1 (+- 0.02) & 0.3 (+- 0.02) \\
Average genome size in Gbp& 1.1 (+- 0.5) & 0.3 (+- 0.1) \\
Average gene length & 23000 (+- 5000) & 3370 (+- 50) \\
\botrule
\end{tabular}}{Values are averages across all genomes of a data group}
\end{table}

During data generation, we only queried for the longest transcript of each gene and did not include FASTA sequences that had no functional annotation (explanation why this is a sign of error?). As Geenuff checks the genomic annotations for potential errors like missing UTR sections and marks areas as potentially erroneous, we were able to effectively blank out those bases during training. For the training, we divided each continous genomic sequence into 20.000 or 30.000 bases long subsequences, which are then used as input or label for the neural network. We appended zero padding if the subsequences are not long enough. If a subsequence is fully marked as erroneous, we do not include it in the training or evaluation datasets.

The selection of training genomes requires balancing multiple conflicting tradeoffs. On the one hand, we do want to train with as much data as possible, but not all data has a good enough quality to enable a powerful generalization. It is also very desirable to have a broad spread in phylogenetic distance as well as genome size and average gene length as the model is increasingly unlikely to generalize well beyond the borders given by the training data. However, if the genomic patterns inside the data are too different from each other, it may be difficult for the model to generalize as well. It was also important to us to get experimental results within a couple of days as we tested many different data and model configurations, which limited the overall size of the training set and the model. 
	
We employed an iterative approach to effectively select a proper set of training genomes. We first started with 3-4 genomes that were almost certain to be of the highest quality. We then evaluated the performance of the resulting model on all genomes individually (including the training genomes) and looked for candidates to add to the training set or to remove from it. This process was repeated multiple times concurrently with the model search until we saw no more room for substantial improvement given our computational constraints.

Once a set of training genomes was selected, we generated a validation set from it by splitting of 20\% of the FASTA sequences above and below the N90 of each genome, ensuring a proper distribution of large and short sequences in both the training and validation set. The validation set was only used to roughly judge the performance of our models during training. Evaluation of the annotations on all training and validation genomes was done regularly, as we ultimately only cared about the performance on fully unknown genomes.




Model evaluation during training was done with a metric that we call the {\it Genic F1}. This score is very similar to a classic F1 score calculated across multiple classes, except that it takes into account that the data is 

\subsection{Model Architecture and Usage}
	We use a 4-layer deep stacked BLSTM network with 256 units per layer and layer normalization (citation ? ) between each BLSTM layer to produce the predictions in the form of a base pair wise classification. The model consists of circa 5.4 million parameters and was implemented with the deep learning library keras (citation) on top of TensorFlow (citation). All relevant hyperparameters can be found in Table \ref{Tab:02} or in the Helixer source code repository. 
	
The model was trained with 10 bases of genomic sequence as input during each time step and produces individual predictions for each of those 10 bases simultaneously. This grouping enabled us to train with far longer sequences than normally done.

- how we train and validate the model
- input of longer sequences
- overlapping
- ensembling

\subsection{Evaluation of Augustus}
What Marvin did, why we don't use Genic F1 for comparison, etc


\begin{table}[!t]
\processtable{Data arrays generated and used by Helixer\label{Tab:01}} {
\begin{tabular}{@{}ll@{}}
\toprule Name & Information \\
\midrule
Input & Genomic sequence in a one hot encoding\\
	  & for (A, T, C, G), otherwise different, see suplementary .. ()\\
Output & Labels in a 4-dimensional one hot encoding \\
Sample weights & One of $\{0,1\}$; whether there is an error at a base\\
Gene lengths & The length of a gene at a base or 0 if intergenic \\
RNASeq score & A score in the interval $...$ to rank the coherence of the \\
		     & annotation with RNASeq information \\
\botrule
\end{tabular}}{}
\end{table}

\begin{table}[!t]
\processtable{Hyperparameters\label{Tab:02}} {
\begin{tabular}{@{}ll@{}}
\toprule Name & Value \\
\midrule
Layers & 4\\
Units per layer & 256\\
Learning rate & 1e-4\\
Optimizer & Adam\\
\botrule
\end{tabular}}{}
\end{table}

text\vspace*{1pt}

\begin{itemize}
\item for bulleted list, use itemize
\item for bulleted list, use itemize
\item for bulleted list, use itemize\vspace*{1pt}
\end{itemize}

Text Text Text Text Text Text  Text Text Text Text Text Text Text
Text Text  Text Text Text Text Text Text.
Figure~2\vphantom{\ref{fig:02}} shows that the above method  Text

\citealp{Boffelli03} might want to know about  text text text text
Text Text Text Text Text Text  Text Text Text Text Text Text Text
Text Text  Text Text Text Text Text Text.

\end{methods}

\begin{figure}[!tpb]%figure1
\fboxsep=0pt\colorbox{gray}{\begin{minipage}[t]{235pt} \vbox to 100pt{\vfill\hbox to
235pt{\hfill\fontsize{24pt}{24pt}\selectfont FPO\hfill}\vfill}
\end{minipage}}
%\centerline{\includegraphics{fig01.eps}}
\caption{Caption, caption.}\label{fig:01}
\end{figure}

%\begin{figure}[!tpb]%figure2
%%\centerline{\includegraphics{fig02.eps}}
%\caption{Caption, caption.}\label{fig:02}
%\end{figure}

\begin{table}[!t]
\processtable{Experimental Results and Comparison\label{Tab:03}} {
\begin{tabular}{@{}lll@{}}
\toprule & EnsemblMetazoa & JGI Phytosome\\
\midrule
Augustus & 0.62 (+-0.02) & 0.83 (+- 0.03)\\
Best Helixer model & 0.62 (+-0.02) & 0.72 (+-0.02) \\
Best model + varied input length & 0.62 (+-0.02) & 0.72 (+-0.02) \\
Best model + varied input length + overlapping & 0.62 (+-0.02) & 0.72 (+-0.02) \\
Varied input length + overlapping + ensembling & 0.62 (+-0.02) & 0.72 (+-0.02) \\
\botrule
\end{tabular}}{Values are averages in CDS F1 (?) across all validation and test genomes of the \\ respective group.}
\end{table}

Text Text Text Text Text Text  Text Text Text Text Text Text Text
Text Text  Text Text Text Text Text Text.
Figure~2\vphantom{\ref{fig:02}} shows that the above method  Text
\citealp{Boffelli03} might want to know about  text text text text


\section{Discussion}

Text Text Text Text Text Text  Text Text Text Text Text Text Text
Text Text  Text Text Text Text Text Text.
Figure~2\vphantom{\ref{fig:02}} shows that the above method  Text
Text Text Text  Text Text Text Text Text Text  Text Text.
\citealp{Boffelli03} might want to know about  text text text text
Text Text Text Text Text Text  Text Text Text Text Text Text Text
Text Text  Text Text Text Text Text Text.
Figure~2\vphantom{\ref{fig:02}} shows that the above method  Text
Text Text Text  Text Text Text Text Text Text  Text Text.
\citealp{Boffelli03} might want to know about  text text text text
Text Text Text Text Text Text Text Text Text Text.




Table~\ref{Tab:01} shows that Text Text Text Text Text  Text Text
Text Text Text Text. Figure~2\vphantom{\ref{fig:02}} shows that
the above method Text Text. Text Text Text  Text Text Text Text
Text Text. Figure~2\vphantom{\ref{fig:02}} shows that the above
method Text Text. Text Text Text  Text Text Text Text Text Text.
Figure~2\vphantom{\ref{fig:02}} shows that the above method Text
Text.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%     please remove the " % " symbol from \centerline{\includegraphics{fig01.eps}}
%     as it may ignore the figures.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\section{Conclusion}

Text Text Text Text Text Text  Text Text Text Text Text Text Text
Text Text  Text Text Text Text Text Text.
Figure~2\vphantom{\ref{fig:02}} shows that the above method  Text
Text Text Text\vspace*{-10pt}


\section*{Acknowledgements}

Text Text Text Text Text Text  Text Text.  \citealp{Boffelli03} might want to know about  text
text text text\vspace*{-12pt}

\section*{Funding}

This work has been supported by the... Text Text  Text Text.\vspace*{-12pt}

%\bibliographystyle{natbib}
%\bibliographystyle{achemnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{abbrv}
%\bibliographystyle{bioinformatics}
%
%\bibliographystyle{plain}
%
%\bibliography{Document}


\begin{thebibliography}{}

\bibitem[Denton {\it et~al}., 2019]{Denton19}
Denton,A., Stiehler,F. (2019) Article title, {\it Journal Name}, {\bf 199}, 133-154.

\bibitem[Howe KL {\it et~al}., 2019]{Howe19}
Howe KL, Contreras-Moreira B, De Silva N, Maslen G, Akanni W, Allen J, Alvarez-Jarreta J, Barba M, Bolser DM, Cambell L, Carbajo M, Chakiachvili M, Christensen M, Cummins C, Cuzick A, Davis P, Fexova S, Gall A, George N, Gil L, Gupta P, Hammond-Kosack KE, Haskell E, Hunt SE, Jaiswal P, Janacek SH, Kersey PJ, Langridge N, Maheswari U, Maurel T, McDowall MD, Moore B, Muffato M, Naamati G, Naithani S, Olson A, Papatheodorou I, Patricio M, Paulini M, Pedro H, Perry E, Preece J, Rosello M, Russell M, Sitnik V, Staines DM, Stein J, Tello-Ruiz MK, Trevanion SJ, Urban M, Wei S, Ware D, Williams G, Yates AD, Flicek P. (2019) Ensembl Genomes 2020-enabling non-vertebrate genomic research. {\it Nucleic Acids Research 2019} https://doi.org/10.1093/nar/gkz890

\end{thebibliography}
\end{document}
