\documentclass{bioinfo}
\copyrightyear{2015} \pubyear{2015}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{Manuscript Category}
\usepackage{url}
\usepackage{placeins}
\begin{document}
\firstpage{1}

\subtitle{Genome analysis}

\title[short Title]{Helixer: Cross-species Gene Annotation Of Large Eucaryotic Genomes Using Deep Learning}
\author[Sample \textit{et~al}.]{Felix Stiehler\,$^{\text{\sfb 1,}}$$^\dagger$, Alisandra Denton\,$^{\text{\sfb 1,}}$$^\dagger$\, Marvin Steinborn\,$^{\text{\sfb 1}}$, Daniela Dey, Stephan Scholz, Andreas Weber\,$^{\text{\sfb 1,}*}$}
\address{$^{\text{\sf 1}}$Institue for Plant Biochemistry, Heinrich-Heine-University, Dusseldorf, D-40225, Germany}

\corresp{$^\dagger$Authors contributed equally. \newline $^\ast$To whom correspondence should be addressed.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\abstract{
\textbf{Motivation:} Current state-of-the-art tools for the \textit{de novo} annotation of 
genes in eukaryotic genomes have to be specifically fitted for each species and still often 
produce annotations that can be improved much further. The fundamental algorithmic 
architecture for these tools has remained largely unchanged for about two decades, limiting
learning capabilities. Here, we set out to improve the cross-species annotation of
genes from DNA sequence alone with the help of deep learning. The goal is to eliminate 
the dependency on a closely related gene model while also improving the predictive
quality in general with a fundamentally new architecture.\\
\textbf{Results:} We present Helixer, a framework for the development and usage of 
a cross-species deep learning model that improves significantly on performance and
generalizability when compared to more traditional methods. We evaluate our 
approach by building a single model for the annotation of 186 animal genomes as 
well as one for 51 plants. Our predictions are shown to be much less sensitive to
the length of the genome than those of a current state-of-the-art tool. We also 
present two novel post-processing techniques that each worked to further strengthen 
our annotations and show in-depth results of an RNA-Seq based comparison of our 
predictions.\\
\textbf{Availability:} The source code of this work is available at 
\url{https://github.com/weberlab-hhu/HelixerPrep} [change] under the GNU General 
Public License v3.0.\\
\textbf{Contact:} \href{andreas.weber@hhu.de}{andreas.weber@hhu.de}\\  
\textbf{Supplementary information:} Supplementary data are available at 
\textit{Bioinformatics} online.
}

\maketitle

\section{Introduction}
The annotation of genes in DNA is fundamental for many downstream tasks, which 
depend on high quality annotations. Gene annotation can be performed at different levels 
of precision, from simple coding -- non-coding classification to detailed structural 
labeling. Because of the sheer size of genomes alone, manual gene annotation is 
generally intractable. Instead, researchers can use pipelines such as 
Maker \citep{}, PASA \citep{} or those offered by genomic database providers like NCBI 
\citep{thibaud2013eukaryotic} and Ensembl \citep{aken2016ensembl}. 
These pipelines integrate experimental data (from e.g. RNA-seq 
or proteogenomics) with homologous sequences in the database and ab-initio gene 
predictions. The latter is an attractive approach, because it is cheap and fast. 
State-of-the-art performance is achieved by higher order hidden markov models 
(HMMs), such as Genscan \citep{burge1997prediction}, AUGUSTUS \citep{stanke2003gene} 
or SNAP \citep{johnson2008snap}. Their accuracy, however, leaves room for 
improvement. By encoding possible states and transitions in a probalistic model, 
designers of HMMs assume structure in the sequence that may limit its predictive 
power. In practice HMMs have trouble generalizing across species and it seems 
actual learning of underlying patterns is limited despite the fundamental ability 
to do so being present \citep{}. 

In the last decade, deep neural networks (DNN) have been applied with great success 
in many areas of statistical modelling, including biology 
\citep{ching2018opportunities}. For sequence data, such as DNA, speech or text, a 
special kind of recurrent neural network (RNN) called long-short term memory (LSTM) 
\citep{hochreiter1997long} is an established building block for many different 
architectures. LSTM units can also be used to process sequential input starting 
from both ends, forming a bi-directional LSTM (BLSTM). It has also been shown that 
HMMs can be successfully combined with DNNs \citep{liu2016novo, liu2016pedla}.

For the purpose of gene annotation, RNNs have already shown promising results. 
\citep{choudharypredicting} carries out preliminary explorations on the potential 
of BLSTMs for cross species gene prediction and trains his model on human genes to 
test it later on two more species. DeepAnnotator \citep{amin2018deepannotator} uses 
bidirectional LSTMs (BLSTM) for gene finding in prokaryotes. Gene prediction in 
prokaryotes is considered more amenable than in eukaryotes, as genes in prokaryotes 
are proportionately more frequent in the genome, feature simpler control structures 
and do not use splicing \citep{wang2004brief}. DanQ \citep{quang2016danq} proposes 
the use of a BLSTM after a convolutional neural net (CNN) to find detailed motifs in 
the human genome. DeePromotor \citep{oubounyt2019deepromoter} trains a similar 
architecture for the recognition of promotor regions. Recently, several groups 
\citep{jaganathan2019predicting, wang2019splicefinder} successfully used CNNs to 
find splicing sites.

In this work we present Helixer, a novel prototype software for training and 
utilizing a general purpose DNN for the ab-initio cross-species gene annotation of 
large eukaryotic genomes using only DNA sequence as input. Our model is trained to 
differentiate between 4 regions: Intergenic, Untranslated (UTR), Coding (CDS) and 
Intron. We demonstrate the effectiveness of this approach by training a single model 
for the annotation of a large set of genomes from the kingdoms metazoa and 
viridiplantae, respectively, which we will call {\it animals} and {\it plants} from 
now on. We worked with the full data of 192 animal genomes and 60 plant genomes. 
Both the ability to generalize across genomes as well as the scope of the evaluation 
greatly surpasses all other work in this field. The source code and all input data 
is publically available.

\begin{methods}
\section{Methods}
\subsection{Datasets}
The foundation of our work are 192 animal and 60 plant genomes. The data of each 
genome consists of the latest publically available genomic assembly in form of a 
FASTA file and the latest annotation in the GFF format. One genome for each species of animal 
in EnsemblMetazoa 45 \citep{howe2020ensembl} as well as all non-embargoed plant
species from the JGI Phytosome 13 database \citep{goodstein2012phytozome}; exact 
genomes used are list here \ref{}. % add a supplemental table with names and versions
Data was downloaded on 29/03/2019 and 15/10/2019 for plants and animals respectively. 
Both data groups were used seperately during training as well as during evaluation. 

Both to divide genomes into training, validation and test sets, as well as to understand
performance, we needed a way to evaluate genome and annotation quality.
Metadata was generated for each genome, including: basic length and composition statistics \citep{}; % quast
k-mer content \citep{}; % jellyfish
conserved gene content of the genome, transcriptome, and proteome \citep{}; % BUSCO, files generated with gffread
and finally simple counts of genic features from the gff files. In particular, 
we considered the following features of the metadata when judging the quality of a genome and it's 
annotation: assembly contiguity (N50 / genome size); retention in the proteome 
of detected conserved orthogroups (BUSCOs) that were identified in the genome; and
the presence of UTRs and alternative splicing as estimated by the gff feature-counts.
Finally this metadata was combined with researcher knowledge of model species to guide genome
selection as necessary.

At the start of our work we split both genome groups into {\it training genomes} and 
{\it test genomes}. The test genome groups are made up of 19 and 6 genomes, 
respectively, and were chosen to be of decent quality and diversity based on collected 
metadata while also representing a broad phylogenetic spread. 

For both feasibility and iterative cross-species generalization validattion, we
selected only some of the remaining genomes for direct training.
The selection of training genomes also requires balancing multiple conflicting 
tradeoffs. On the one hand, we do want to train with as much data as possible, but 
not all data has a good enough quality to enable a powerful generalization. It is 
also very desirable to have a broad spread phylogenetically as well as in genome 
size and average gene length as the model is increasingly unlikely to generalize 
well beyond the borders given by the training data. However, it also may be difficult 
for the model learn and generalize, if the genomic patterns inside the data are 
too different from each other. It was also important for us to limit the size of
our training genomes set to be small enough that enough that we could get experimental 
results within a couple of days and thereby be able to test many different data and 
model configurations.

We employed an iterative approach to effectively select a proper set of training 
genomes. We first started with 3-4 genomes that were expected to be of the 
highest quality. We then evaluated the performance of the resulting model on all 
genomes individually (including the training genomes) and looked for candidates to 
add to the training set or to remove from it. This process was repeated multiple 
times concurrently with the model search until we saw no more room for substantial 
improvement given our computational constraints.

We call the genomes that were ultimatily not used during training from the original 
split the {\it validation genomes}. While we did split off a group of test genomes 
at the beginning of our work, we ended up only reporting on the generalization 
performance on the combined set of validation and test genomes as the validation 
genomes are by far the largest set. We also did not find a noticible difference in 
performance between those two groups. 

Once a set of training genomes was selected, we further split the sequences therein into a {\it 
training and validation set}. This split of the training genomes was done to get a 
quicker sense of the generalization capabilities and was used primarily during 
training. We split off the validation set by selecting 20\% of the FASTA sequences 
above and below the N90 of each training genome, ensuring a proper distribution of 
large and short sequences in both sets and a split on the chromosome level. 
Evaluation of the annotations on all training and validation genomes was done 
regularly after a promising model candidate was found based on its validation set 
performance. Ultimately we used the cross-species performance on all 
validation genomes as the decisive measure of model quality.

\subsection{Data Pre-processing}
We first pre-process and store the raw genomic information by using GeenuFF 
\citep{denton2019}. GeenuFF is a tool for checking and exploring genomic data, that 
stores all information inside a SQL database. Training and evaluation-ready data is 
generated by querying this database and then transforming the returned data into a 
numerical format suitable for machine learning. The encoding of the genomic sequence 
is done in line with the IUPAC nucleic acid notation and the functional annotation 
used as labels during training is transformed to a one hot encoding with the four 
classes {\it Intergenic}, {\it UTR}, {\it CDS} and {\it Intron}. See also Table 
\ref{tab:arrays} for a more detailed description of the generated data types.

\begin{table}[!t]
\processtable{Data group statistics for all data\label{tab:statistics_all}} {
\begin{tabular}{@{}lll@{}}
\toprule & Animals & Plants\\
\midrule
Average genome size in Gbp& 2.936 (+- 1.562) & 0.787 (+- 0.995) \\
Average gene length & 31,223 (+- 13,974)& 3,368 (+- 1,510)\\
Geenuff error rate & 0.311 (+- 0.129) & 0.351 (+- 0.249) \\
Fraction of class Intergenic  & 0.777 (+- 0.042) & 0.799 (+- 0.089) \\
Fraction of class UTR & 0.006 (+- 0.006) & 0.017 (+- 0.016) \\
Fraction of class CDS & 0.016 (+- 0.014) & 0.085 (+- 0.071) \\
Fraction of class Intron & 0.201 (+- 0.035) & 0.099 (+- 0.057) \\
\botrule
\end{tabular}}{{\it Note:} Values are averages of the individual values of each 
genome in a data group. All statistics except the average gene lengths exclude
FASTA sequences without a gene and any 20,000 bp subsequences that were masked as completely 
erroneous. Each strand of DNA was counted seperately. The gene length was 
determined by the length of the pre-mRNA of the longest protein at each
loci. Brackets show the standard deviation.}
\end{table}

During data generation, we query for the transcript with the longest protein of each gene and 
disregard FASTA sequences that have no functional annotation as it is ambiguous
whether such sequences contain no genes or were simply not annotated in the reference. GeenuFF checks the 
genomic annotations for potential errors during import and is able to mark those 
areas. We use this information to effectively blank out those bases during training 
by using the sample weights described in Table \ref{tab:arrays}. The vast majority 
of masked bases lie in the intergenic region as the most prevelant error is a 
missing UTR and GeenuFF marks a potentially large intergenic region for it. Table 
\ref{tab:statistics_all} shows statistics about the masking. 

For the training itself, we divide each continous genomic sequence into 20,000 bp 
long subsequences, for which one-hot vectors of the base pairs and annotations 
are generated and respectively used as input and label for the neural network 
together with the sample weights. We append zero padding if the sequences (or
sequence ends) are shorter than 20,000 bp. 
If a subsequence is fully marked as erroneous, we exclude it from all analyses. 

\subsection{Metrics}
We mainly use two metrics to judge model performance. The {\it Genic F1} is our 
primary metric that provides the most comprehensive picture of annotation quality 
in one number. The {\it Subgenic F1} is similar to the Genic F1, except that it does 
not take UTR predictions into account. This is done for comparability with AUGUSTUS 
and further explained in Section \ref{sec:augustus}. Both metrics work by summing up 
True Positives (TP), False Positives (FP) and False Negatives (FN) of each 
considered class before calculating precision and recall from these values and 
combining the two into a F1 score. Neither metric takes the values of the 
intergenic class into account, as this class is very abundant and appears to be by 
far the easiest to predict. (See Table \ref{tab:statistics_all} for the class 
distribution and Tables S1 and S2 for a more in-depth report our model performance 
including the intergenic class). In the case of the Genic F1, this means that 
effectively only the TP of the intergenic class are disregarded. The metrics 
essentially provide a weighted mean of the performances in the considered classes. %?? hmmm, but this is not a weighted mean across classes is it? Do introns not weigh more heavily?

In Figure \ref{fig:main_results}c we also report on the overall base pair level 
accuracy besides the Genic F1.

\subsection{Model Architecture and Training}
\label{sec:model}
We use a 4-layer deep stacked BLSTM network with 256 units per layer and layer 
normalization \citep{ba2016layer} between each BLSTM layer to produce the 
predictions in the form of a base pair wise classification. The model consists of 
circa 5.4 million parameters and was implemented with the deep learning library 
keras \citep{chollet2015keras} on top of TensorFlow \citep{abadi2016tensorflow}. We 
tested multiple different architectures before arriving at this model configuration, 
including convolutional neural networks (CNN) and hybrid architectures. We also use 
class weigths as the classes of the input data are both unbalanced vary greatly 
between animals and plants. 
For example, the tradeoff between the number of intergenic and intronic bases is 
quite different in both groups (see Table \ref{tab:statistics_all}). 

A class wise evaluation including the calculation of the Genic 
F1 score is performed on the validation set after every epoch. The best model
from any training run was selected as the model with the highest Genic F1 after either
maximum epochs were reached or Genic F1 stopped improving and training was interrupted.

The hyperparameters were optimized by a combination of manual and automatic 
optimization. Automatic optimization was carried out by using either the TPE 
algorithm \citep{bergstra2011algorithms}, random search or grid search depending on 
the situation. We used NNI \citep{nni2019} to facilitate the search. All relevant 
hyperparameters can be found in Table S3 and S4 or in the Helixer source code 
repository. 
	
Our final model was trained with 10 bases of genomic sequence as input during each 
time step and produces individual predictions for each of those 10 bases 
simultaneously. This grouping enabled us to train effectively with far longer 
sequences than usual. 
We compare our final models to a dilated CNN (dCNN) baseline as this kind of architecture is 
widely used when working with DNA data as input. Details of the neural architecture 
search are given in Section 6 of the Supplements.

\subsection{Inference Techniques}
We also used multiple techniques to improve the prediction quality after the 
training was done. One very effective way for genomes with larger genes was to input 
longer sequences during inference time than during training. This was done for all 
animal genomes except the invertebrates. The input sequences were up to 10 times 
longer, depending on the phylogenetic group and assembly quality. This also demonstrates the 
ability of our model to generalize as it is able to make successful prediction on 
far longer sequences than it has ever seen. The 
concrete lengths were chosen to keep the typical average gene length roughly 
proportional to the length of the sequence input. For more implementation details 
on this see Section 3 of the Supplements or the source code.
	
The final predictions of a single model are constructed by overlapping predictions, 
that were made from a sliding window and then cropped to a core sequence. This was done 
to strongly reduce a typical drop in performance of the models towards the beginning 
and end of each sequence (see Figure \ref{fig:main_results}c). It also improves the 
average model performance in general by providing the model with multiple different 
starting points. The different overlapping sequences were combined by averaging 
the individual softmax values of each base. The figures in Section 4 of the 
Supplements show the effect of overlapping for each genome, ordered by N75. We found 
that overlapping tends to work best if the genomes are not very fragmented and we 
used it for both animals and plants.

A model ensemble with 8 components was used to generate the final predictions for 
each species. For this, we performed 4 separate animal and plant training runs and 
selected two models each, specifically the model with the best precision and the  % please check / epoch model / version something
other with the best recall. The model with the best Genic F1 was always best in one of 
those. This was done to increase the diversity of the model ensemble. As with the 
overlapping, the fusion of the 8 individual predictions was done by averaging the 
softmax values of each base pair prediction.

\subsection{Evaluation of AUGUSTUS} 
\label{sec:augustus}
Due to the practical time constraints for retraining and running, we decided to compare our Helixer
models only to one existing {\it de novo} gene caller, the popular gene caller, Augustus.
Typical usage requires retraining Augustus for each species, with the exception of a
few lucky cases where a model for a close relative is already available. To scale this for 
the large plant and animal datasets, we used protein-homology to create a training set and
therefore could train and evaluate only models without UTRs. 

Orthologs of highly conserved generally single-copy genes were identified in each genome using
BUSCO. The {\it viridiplantae} set was used for plants and the {\it metazoa} set for animals.
The training genbank files were generated directly by BUSCO, by utilizing the ``--long"  % inline code?
parameter. For plants, the entire retraining could be performed as above; however, for animals
we randomly selected only half the BUSCO-generated training set, which resulted in a training set 
size and runtime comparable to the plants (~400 genes and several days per species). For animals
the training was carried out with the subsetted training file and using the following scripts provided
by Augustus. An untrained model was setup with "new\_species.pl", and the model was fit by running
"etraining" before and after the major hyper-parameter optimization with "optimize\_augustus.pl".
Using the trained model for each species, we ran the main prediction ("augustus") with ``--UTR=off"
and ``--gff3=on". The gffs produced by Augustus were imported into GeenuFF and exported as h5
files in the same manor as the reference, allowing direct comparison with reference and 
Helixer predictions. While tenable for some 237 species, this method allowed neither training
nor prediction of UTR regions with Augustus, so the metric Subgenic F1, which excludes UTR,
was used for all comparisons between Augustus and Helixer.

\subsection{Evaluation against independent RNAseq data}
\label{sec:rnaseq}
The reference annotations were created with existing tools and largely with a pipeline incorporating
{\it de novo} gene predictions with RNAseq and homology data. The references, like any
data, are expected to contain errors; but here we were particularly concerned because
the {\it de novo} gene caller used by the references was Augustus for many species (...)
% todo list and cite examples (above and below)
and a fundamentally similar HMM-based tool for many more (...), which would potentially bias 
reference based comparisons of Helixer's and Augustus' performance. 

We therefore downloaded and processed public RNAseq data to obtain an independent option 
for evaluating model performance. We selected three each of plant and animal genomes for
detailed evaluation with RNAseq. These were selected to have relatively good 
({\it Manihot esculenta} and {\it Papio anubis}), 
typical ({\it Medicago truncatula} and {\it Equus caballus}), 
and poor ({\it Theobroma cacao} and {\it Petromyzon marinus})
performance compared to Augustus \ref{} % supp phylogenies maybe?
within our generalizable range (i.e. excluding algae and
invertebrates). Selections were further constrained by the availability of stranded RNAseq
data. 

% todo, add versions to everything below
For each of these six species the following search was performed on Sequence Read Archive
'(("<species name>"[Organism] OR <species name>[All Fields]) AND stranded[All Fields]) 
AND ("biomol rna"[Properties] AND "library layout paired"[Properties])'. In so far as 
more than 50 samples were identified, every Nth sample was selected so that about 50
samples were chosen for further processing, as listed here \ref{}. % todo make a supp table
Each sample was quality controlled with FastQC \citep{}, before and after trimming
with Trimmomatic \citep{}, and then mapped to the genome with Hisat2 \citep{}. 
Samtools \citep{} was used for the sorting and indexing that was necessary both for
RNAseq mapped metric collection with PicardTools \citep{} and custom downstream 
processing below. MultiQC \citep{} was used to summarize the results. The above was 
automated and the results visualized with the code here (... make available and link to RNAseq stuff).
Samples were filtered to those that had {\it relatively} high mapping rates, particularly to
exonic regions; that had low 3' bias, normal FastQC and Trimmomatic stats, and that
were confirmed to be stranded with 2nd strand as sense strand. If more remained, seven
of the high quality samples were selected randomly.

Finally, the selected, quality, mapped RNAseq samples were merged and quantified to
get the coverage (number of reads matching; i.e. cigar =, M or X),
and spliced coverage (number of reads with gap or splice; i.e. cigar N or D)
for every base pair in the genomes as implemented in the "training\_rnaseq.py"
script.

\begin{table}[!t]
\processtable{Data arrays generated and used by Helixer\label{tab:arrays}} {
\begin{tabular}{@{}ll@{}}
\toprule Name & Information \\
\midrule
Input & Genomic sequence in the 4-dimensional IUPAC encoding\\
	  & (one hot encoding for non-ambiguous bases) \\
Output & Labels in a 4-dimensional one hot encoding representing \\
      & the classes {\it Intergenic}, {\it UTR}, {\it CDS} and {\it Intron}\\
Sample weights & One of $\{0,1\}$; whether there is an error at a base\\
\botrule
\end{tabular}}{{\it Note:} An intron inside a UTR region is encoded as intron and 
can be inferred from the prediction order.}
\end{table}

\end{methods}

\begin{figure*}[!tpb]
\label{fig:main_results}
\centerline{\includegraphics[width=\textwidth]{images/main_results}}
\caption{Main overall results and other investigations. The top row shows the animal
 data; the bottom row is concerned with the plants. Yellow dots are for AUGUSTUS and 
the blue ones are for Helixer. {\bf a)} Swarm - and boxplots showing the Subgenic F1 
scores for all genomes that were not used during training of Helixer, which is 
equivalent to all validation and test genomes. Each dot represents the prediction 
performace for one genome. The median prediction scores are shown by the red line. 
Details of how AUGUSTUS was used are given in Section \ref{sec:augustus}. {\bf b)} 
Scatterplots showing the prediction performances measured in Subgenic F1 by genome 
length. The regression line is shown with a 95\% confidence interval. {\bf c)} The 
average Genic F1 (in red) and basepair wise accuracy (in purple) with respect to 
the position in the 20000 basepair long input sequences. The dashed line shows the 
same without overlapping. Each value is the average performance of a 200 basepair 
long subsection by one of the models of the final ensemble. See Section 4 for the 
Supplements shows the effects of overlapping for each individual species.}
\end{figure*}

\begin{table}[!t]
\processtable{Summary of Experimental Results\label{tab:results}} {
\begin{tabular}{@{}lll@{}}
\toprule & Animals & Plants\\
\midrule
AUGUSTUS & 0.632 & 0.757 \\
Dilated CNN Baseline & 0.666 &  0.802 \\
Best Helixer model & 0.77  & 0.833  \\
Best model + varied input length & 0.834  & - \\
Best model (+ varied input length) + overlapping &  0.844  & 0.843  \\
(Varied input length +) overlapping + ensembling & 0.868  & 0.863  \\
\botrule
\end{tabular}}{{\it Note:} Values are the median in Subgenic F1 across all 
validation and test genomes of the respective group. Varied input length was only 
used in the animal case. The best model was chosen out of the eight models of the 
ensemble for having the best performance in the reported setup.}
\end{table}

\section{Results}
Figure \ref{fig:main_results}a shows the side-by-side comparison of the distribution 
of performances on all non-training genomes in the animal and plant case by Subgenic 
F1. In the case of Helixer, these scores represent cross-species predictions. We 
also compare the median performances of different configurations of Helixer with 
AUGUSTUS and a dilated CNN baseline in Table \ref{tab:results}. The results show a 
clear improvement over the AUGUSTUS both in median performance and spread. We also 
significantly outperform a baseline constructed of a dilated CNN architecture.

Our models, however, tend to perform less well for the very smallest and largest 
genomes or species that are phylogenetically the furthest away from our training 
genomes. This is the case for both animals and plants and is visualized by the 
Figures S1 and S3. We do not, for example, consistently predict very well on the 
algae as well as the non-avian reptiles. While one of the the algae were included 
in our training genomes it accounted for a tiny proportion of the total training 
data and the non-avian reptiles were not included at all.
 
AUGUSTUS outperforms us on some of the smallest genomes, but falls off much more 
drastically as the genomes get larger. The difference in prediction quality is 
especially strong for mammals, which tend to have big genomes with very long genes 
as well as the largest plants. Figures S2 and S4 display the comparison to AUGUSTUS 
by phylogenetic position.

Two techniques were used during inference to improve performance and limit model 
bias. The usage of longer input lengths helped especially for genomes that tend to 
have longer genes and was enabled by our model architecture being a relatively 
simple LSTM stack without any fully connected layers on top. We also constructed the 
final predictions out of a overlapping ones, which greatly helped to reduce 
prediction bias in most genomes. To our knowledge, neither technique has been used 
before in a model developed for gene annotation. 

\begin{figure*}[!h]
\label{fig:cov_example_main}
\centerline{\includegraphics[width=\textwidth]{images/cov_examples/cov_example_main}}
\caption{Four example helixer predictions in the context of RNAseq data, the reference,
and Augustus' prediction for {\it M. esculenta}. The examples were chosen so
that a) Helixer had high accuracy against the reference and the reference
was supported by the RNAseq data, b) Helixer had high accuracy but the 
reference was not supported by the RNAseq data, c) Helixer had low accuracy
and the reference was supported by the RNAseq data, and d) helixer had low
accuracy but the reference was not supported by RNAseq data. Plotability was
a major secondary consideration. Each subplot shows from top to
bottom i) the natural log of the coverage (``cov", solid) and spliced coverage 
(``sc", dotted) + 1, ii) the reference annotation in matrix form, iii) 
Augustus' predictions in matrix form, and iv) Helixer's predictions. The reference
and Augustus have either 0 (white) or 1 (black) for each base pair and category, while
Helixer emits a probability from 0-1 represented via gray-scale. ``Ntrn" stand
for intron, and ``IG" stands for intergenic.
}
\end{figure*}

As an independent evaluation and to get a better qualitative understanding of
performance we compared Helixer predictions, Augustus predictions and the
reference to RNAseq coverage data. RNAseq was processed for three each of plants
and animals chosen because they had varied performance quality relative to Augustus. 
RNAseq coverage provides support for an 
exonic annotation (UTR or CDS), spliced coverages provides support for an 
annotation of intron, and neither coverage nor spliced coverage is expected 
for intergenic annotations. Looking at selected subsequences (Figure \ref{fig:cov_example_main}) % and supplemental figures.
we identified cases where RNAseq supported both the annotations of the reference 
and Helixer (a), neither (b), the reference, but not Helixer (c),
and Helixer, but not the reference (d). 

Helixer models do not yet have postprocessing
to make finalized single predictions, but instead output basewise probabilities.
We see that the model exibits higher uncertainty around transitions between annotation 
classes, for instance between UTR and CDS, or more dramatically between UTR and 
intergenic even where Helixer predictions closely match the reference and RNAseq data (a). 
Helixer's uncertainty around UTR <-> intergenic transitions may relate to a
fundamentally harder problem (there is no conserved motif at the site as is observed
for splice sites and start/stop codons), lack a single precise biological site \citep{},
or noise in the reference, which we observed relative to the RNAseq data \ref{}. % supplemental UTR figures, 1/species

Helixer models also sometimes showed uncertainty for larger regions.
In some cases, where Helixer did not receive RNAseq support for its highest probability
annotation, it assigned a low but non-trivial probability to the RNAseq-supported 
exon/intron pattern (b and c,hSlA2 supp). However, in the extreme,
there are cases where the Helixer model exibits substantial indecision or confusion 
and shifts gradually between classes with no single class receiving a high probability 
for extended stretches (hSlA2, hSlA3, lSlA3). Notably, in one of the examples ()
the RNAseq shows evidence of alternative splicing; and in another () Helixer's
prediction falls between that of the reference and Augustus.

\begin{figure*}[!h]
\label{fig:average_vs_augustus}
\includegraphics[width=8.6cm]{images/cov_examples/average_vs_augustus}  % should be 8.6 when floats are sorted out right
\caption{Fraction of bp with color-indicated a) coverage and b) spliced coverage of genomic positions 
broken down by the confusion matrix of Augustus and Helixer's predictions. Categories 
are only displayed if they can be meaningfully compared by examining a) coverage or b) spliced
coverage. The displayed fractions are the averages of the individual fractions for the 
six RNAseq-evaluation species. The left-most bars show cases where the two tools agree, 
while the remaining bars show paired conflicts. ``Ntrn" stand
for intron, and ``IG" stands for intergenic.}
\end{figure*}

Coverage and Spliced coverage were broken down by the confusion matrix (of both
Helixer vs Augustus and Helixer vs the reference) for all genomes (Figure \ref{fig:average_vs_augustus}). % H vs A main, H vs R supp
Where tools were in agreement (far left), coverage and spliced coverage closely matched
expectations. Specifically for CDS:CDS and UTR:UTR most base pairs had some, and many
had moderate or high coverage. The same pattern was seen for intron:intron and spliced
coverage. Finally, intergenic:intergenic showed only a small fraction of base pairs with 
any of either coverage or spliced coverage. In all conflicts the amount of RNAseq support
fell between the cases where tools agreed, indicating that all options were at least
capable of finding weak-spots in the other annotations.
In cases of conflict, Helixer's predictions were substantially more consistent with
the RNAseq data than those of Augustus. Specifically, for base pairs where one tool said CDS and
the other intergenic or intron \ref{}, there was more coverage when 
Helixer said CDS. Similarly, for base pairs where one tool said intron and the other 
CDS or intergenic, there was more spliced coverage when Helixer said intron \ref{}. 
These patterns were consistent in direction but varied in magnitude in the individual species,
with the expception of {\it P. marinus} where Helixer and Augustus performed comparably
at differentiating CDS and introns.

RNAseq based comparison of Helixer and the reference was less clear cut.
Averaged across species, helixer's predictions received slightly more support
than the reference when differentiating CDS, UTR and introns from intergenic;
however helixer's predictions received slightly less support than the reference 
when differentiating CDS and UTR from introns. Performance varies between individual
species, from {\it P. marinus}, where the reference receives more support
in every conflict, to {\it T. cacao}, where Helixer models receive equivalent
or more support in every conflict. Interestingly, these are the two species that
were selected because they had (relative to the usual difference) low Subgenic F1 compared to 
Augustus; for the former the RNAseq confirms relatively weak performance for Helixer, while the 
for the latter RNAseq rather indicates a sub-par reference. 

\section{Discussion}
We indtroduce Helixer a novel, deep-learning based tool for gene annotation
prediction. Helixer outperforms AUGUSTUS on base pair wise metrics and on 
consistency with independent RNASeq data. 

We include trained models for plants and animals which achieve 
high prediction accuracy on cross-species gene annotation for broad 
phylogenetic groups (land plants and vertebrates, respectively).
Within these groups, this eliminates the dependency on retraining and the
expertise and data required therefore. Production of comparable, single-method
annotations for broad groups has the potential to greatly facilitate downstream
analyses which suffer when data is heterogeneous \citep{}.

We found our models to be highly sensitive to the training genomes we chose. A 
different set could lead to a significant shift in strengths and weaknesses of the 
model and a larger and more spread out set of high quality genomes could also result 
in a wider range of genomes with decent predictions. For this, more computational 
resources would be required. Simply training with a similar amount of data, 
but for a different group (e.g. invertebrates of fungi) could also be used to 
increase the functional predictive range. 

An avenue for future research could be the addition of RNASeq data as additional 
input. This would bring Helixer on even footing with current tools and could lead 
to real world applicable performance improvements if the results of this work are 
any indication, as deep learning has been shown to excel in a multimodal settings 
\citep{ching2018opportunities}.

Finally, developent of a post-processing method
to go from base pair wise predictions to integrated predictions for whole 
transcripts at each loci could both further improve performance and would
greatly increase real world applications.
\FloatBarrier

\section*{Acknowledgements}

We are very grateful for the support of...

HHU HPC-team, deNBI, Ebenh{\"o}h lab for computational (particularly GPU) resources.

\section*{Funding}

This work has been supported by the... \vspace*{-12pt}


\bibliographystyle{natbib}
\bibliography{literature}

\end{document}
